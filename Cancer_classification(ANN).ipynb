{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxEGcRdgAHuC"
      },
      "source": [
        "# Keras TF 2.0 - Classification Project\n",
        "\n",
        "Let's explore a classification task with Keras API for TF 2.0\n",
        "\n",
        "## The Data\n",
        "\n",
        "### Breast cancer wisconsin (diagnostic) dataset\n",
        "--------------------------------------------\n",
        "\n",
        "**Data Set Characteristics:**\n",
        "\n",
        "    :Number of Instances: 569\n",
        "\n",
        "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
        "\n",
        "    :Attribute Information:\n",
        "        - radius (mean of distances from center to points on the perimeter)\n",
        "        - texture (standard deviation of gray-scale values)\n",
        "        - perimeter\n",
        "        - area\n",
        "        - smoothness (local variation in radius lengths)\n",
        "        - compactness (perimeter^2 / area - 1.0)\n",
        "        - concavity (severity of concave portions of the contour)\n",
        "        - concave points (number of concave portions of the contour)\n",
        "        - symmetry\n",
        "        - fractal dimension (\"coastline approximation\" - 1)\n",
        "\n",
        "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
        "        largest values) of these features were computed for each image,\n",
        "        resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
        "        13 is Radius SE, field 23 is Worst Radius.\n",
        "\n",
        "        - class:\n",
        "                - WDBC-Malignant\n",
        "                - WDBC-Benign\n",
        "\n",
        "    :Summary Statistics:\n",
        "\n",
        "    ===================================== ====== ======\n",
        "                                           Min    Max\n",
        "    ===================================== ====== ======\n",
        "    radius (mean):                        6.981  28.11\n",
        "    texture (mean):                       9.71   39.28\n",
        "    perimeter (mean):                     43.79  188.5\n",
        "    area (mean):                          143.5  2501.0\n",
        "    smoothness (mean):                    0.053  0.163\n",
        "    compactness (mean):                   0.019  0.345\n",
        "    concavity (mean):                     0.0    0.427\n",
        "    concave points (mean):                0.0    0.201\n",
        "    symmetry (mean):                      0.106  0.304\n",
        "    fractal dimension (mean):             0.05   0.097\n",
        "    radius (standard error):              0.112  2.873\n",
        "    texture (standard error):             0.36   4.885\n",
        "    perimeter (standard error):           0.757  21.98\n",
        "    area (standard error):                6.802  542.2\n",
        "    smoothness (standard error):          0.002  0.031\n",
        "    compactness (standard error):         0.002  0.135\n",
        "    concavity (standard error):           0.0    0.396\n",
        "    concave points (standard error):      0.0    0.053\n",
        "    symmetry (standard error):            0.008  0.079\n",
        "    fractal dimension (standard error):   0.001  0.03\n",
        "    radius (worst):                       7.93   36.04\n",
        "    texture (worst):                      12.02  49.54\n",
        "    perimeter (worst):                    50.41  251.2\n",
        "    area (worst):                         185.2  4254.0\n",
        "    smoothness (worst):                   0.071  0.223\n",
        "    compactness (worst):                  0.027  1.058\n",
        "    concavity (worst):                    0.0    1.252\n",
        "    concave points (worst):               0.0    0.291\n",
        "    symmetry (worst):                     0.156  0.664\n",
        "    fractal dimension (worst):            0.055  0.208\n",
        "    ===================================== ====== ======\n",
        "\n",
        "    :Missing Attribute Values: None\n",
        "\n",
        "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
        "\n",
        "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
        "\n",
        "    :Donor: Nick Street\n",
        "\n",
        "    :Date: November, 1995\n",
        "\n",
        "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
        "https://goo.gl/U2Uwz2\n",
        "\n",
        "Features are computed from a digitized image of a fine needle\n",
        "aspirate (FNA) of a breast mass.  They describe\n",
        "characteristics of the cell nuclei present in the image.\n",
        "\n",
        "Separating plane described above was obtained using\n",
        "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
        "Construction Via Linear Programming.\" Proceedings of the 4th\n",
        "Midwest Artificial Intelligence and Cognitive Science Society,\n",
        "pp. 97-101, 1992], a classification method which uses linear\n",
        "programming to construct a decision tree.  Relevant features\n",
        "were selected using an exhaustive search in the space of 1-4\n",
        "features and 1-3 separating planes.\n",
        "\n",
        "The actual linear program used to obtain the separating plane\n",
        "in the 3-dimensional space is that described in:\n",
        "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
        "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
        "Optimization Methods and Software 1, 1992, 23-34].\n",
        "\n",
        "This database is also available through the UW CS ftp server:\n",
        "\n",
        "ftp ftp.cs.wisc.edu\n",
        "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
        "\n",
        ".. topic:: References\n",
        "\n",
        "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction\n",
        "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on\n",
        "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
        "     San Jose, CA, 1993.\n",
        "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and\n",
        "     prognosis via linear programming. Operations Research, 43(4), pages 570-577,\n",
        "     July-August 1995.\n",
        "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
        "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994)\n",
        "     163-171."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYdt_ziGAHuF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd #impoting the required libraries\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15b0QsEHAHuF"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('10_cancer_classification.csv') #reading the csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTQwmhbOAHuG",
        "outputId": "fb04b73f-103b-4f87-beba-dd1cbb2e0d85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 569 entries, 0 to 568\n",
            "Data columns (total 31 columns):\n",
            " #   Column                   Non-Null Count  Dtype  \n",
            "---  ------                   --------------  -----  \n",
            " 0   mean radius              569 non-null    float64\n",
            " 1   mean texture             569 non-null    float64\n",
            " 2   mean perimeter           569 non-null    float64\n",
            " 3   mean area                569 non-null    float64\n",
            " 4   mean smoothness          569 non-null    float64\n",
            " 5   mean compactness         569 non-null    float64\n",
            " 6   mean concavity           569 non-null    float64\n",
            " 7   mean concave points      569 non-null    float64\n",
            " 8   mean symmetry            569 non-null    float64\n",
            " 9   mean fractal dimension   569 non-null    float64\n",
            " 10  radius error             569 non-null    float64\n",
            " 11  texture error            569 non-null    float64\n",
            " 12  perimeter error          569 non-null    float64\n",
            " 13  area error               569 non-null    float64\n",
            " 14  smoothness error         569 non-null    float64\n",
            " 15  compactness error        569 non-null    float64\n",
            " 16  concavity error          569 non-null    float64\n",
            " 17  concave points error     569 non-null    float64\n",
            " 18  symmetry error           569 non-null    float64\n",
            " 19  fractal dimension error  569 non-null    float64\n",
            " 20  worst radius             569 non-null    float64\n",
            " 21  worst texture            569 non-null    float64\n",
            " 22  worst perimeter          569 non-null    float64\n",
            " 23  worst area               569 non-null    float64\n",
            " 24  worst smoothness         569 non-null    float64\n",
            " 25  worst compactness        569 non-null    float64\n",
            " 26  worst concavity          569 non-null    float64\n",
            " 27  worst concave points     569 non-null    float64\n",
            " 28  worst symmetry           569 non-null    float64\n",
            " 29  worst fractal dimension  569 non-null    float64\n",
            " 30  benign_0__mal_1          569 non-null    int64  \n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 137.9 KB\n"
          ]
        }
      ],
      "source": [
        "df.info() # printing dataset information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oshhNy9zAHuG",
        "outputId": "014ed507-ab95-435b-b587-3ee128d48184"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d559050b-1ec0-4c81-bf6e-10abc9724570\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>mean radius</th>\n",
              "      <td>569.0</td>\n",
              "      <td>14.127292</td>\n",
              "      <td>3.524049</td>\n",
              "      <td>6.981000</td>\n",
              "      <td>11.700000</td>\n",
              "      <td>13.370000</td>\n",
              "      <td>15.780000</td>\n",
              "      <td>28.11000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean texture</th>\n",
              "      <td>569.0</td>\n",
              "      <td>19.289649</td>\n",
              "      <td>4.301036</td>\n",
              "      <td>9.710000</td>\n",
              "      <td>16.170000</td>\n",
              "      <td>18.840000</td>\n",
              "      <td>21.800000</td>\n",
              "      <td>39.28000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean perimeter</th>\n",
              "      <td>569.0</td>\n",
              "      <td>91.969033</td>\n",
              "      <td>24.298981</td>\n",
              "      <td>43.790000</td>\n",
              "      <td>75.170000</td>\n",
              "      <td>86.240000</td>\n",
              "      <td>104.100000</td>\n",
              "      <td>188.50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean area</th>\n",
              "      <td>569.0</td>\n",
              "      <td>654.889104</td>\n",
              "      <td>351.914129</td>\n",
              "      <td>143.500000</td>\n",
              "      <td>420.300000</td>\n",
              "      <td>551.100000</td>\n",
              "      <td>782.700000</td>\n",
              "      <td>2501.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean smoothness</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.096360</td>\n",
              "      <td>0.014064</td>\n",
              "      <td>0.052630</td>\n",
              "      <td>0.086370</td>\n",
              "      <td>0.095870</td>\n",
              "      <td>0.105300</td>\n",
              "      <td>0.16340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean compactness</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.104341</td>\n",
              "      <td>0.052813</td>\n",
              "      <td>0.019380</td>\n",
              "      <td>0.064920</td>\n",
              "      <td>0.092630</td>\n",
              "      <td>0.130400</td>\n",
              "      <td>0.34540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean concavity</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.088799</td>\n",
              "      <td>0.079720</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.029560</td>\n",
              "      <td>0.061540</td>\n",
              "      <td>0.130700</td>\n",
              "      <td>0.42680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean concave points</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.048919</td>\n",
              "      <td>0.038803</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.020310</td>\n",
              "      <td>0.033500</td>\n",
              "      <td>0.074000</td>\n",
              "      <td>0.20120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean symmetry</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.181162</td>\n",
              "      <td>0.027414</td>\n",
              "      <td>0.106000</td>\n",
              "      <td>0.161900</td>\n",
              "      <td>0.179200</td>\n",
              "      <td>0.195700</td>\n",
              "      <td>0.30400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.062798</td>\n",
              "      <td>0.007060</td>\n",
              "      <td>0.049960</td>\n",
              "      <td>0.057700</td>\n",
              "      <td>0.061540</td>\n",
              "      <td>0.066120</td>\n",
              "      <td>0.09744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>radius error</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.405172</td>\n",
              "      <td>0.277313</td>\n",
              "      <td>0.111500</td>\n",
              "      <td>0.232400</td>\n",
              "      <td>0.324200</td>\n",
              "      <td>0.478900</td>\n",
              "      <td>2.87300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>texture error</th>\n",
              "      <td>569.0</td>\n",
              "      <td>1.216853</td>\n",
              "      <td>0.551648</td>\n",
              "      <td>0.360200</td>\n",
              "      <td>0.833900</td>\n",
              "      <td>1.108000</td>\n",
              "      <td>1.474000</td>\n",
              "      <td>4.88500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perimeter error</th>\n",
              "      <td>569.0</td>\n",
              "      <td>2.866059</td>\n",
              "      <td>2.021855</td>\n",
              "      <td>0.757000</td>\n",
              "      <td>1.606000</td>\n",
              "      <td>2.287000</td>\n",
              "      <td>3.357000</td>\n",
              "      <td>21.98000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>area error</th>\n",
              "      <td>569.0</td>\n",
              "      <td>40.337079</td>\n",
              "      <td>45.491006</td>\n",
              "      <td>6.802000</td>\n",
              "      <td>17.850000</td>\n",
              "      <td>24.530000</td>\n",
              "      <td>45.190000</td>\n",
              "      <td>542.20000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smoothness error</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.007041</td>\n",
              "      <td>0.003003</td>\n",
              "      <td>0.001713</td>\n",
              "      <td>0.005169</td>\n",
              "      <td>0.006380</td>\n",
              "      <td>0.008146</td>\n",
              "      <td>0.03113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>compactness error</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.025478</td>\n",
              "      <td>0.017908</td>\n",
              "      <td>0.002252</td>\n",
              "      <td>0.013080</td>\n",
              "      <td>0.020450</td>\n",
              "      <td>0.032450</td>\n",
              "      <td>0.13540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concavity error</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.031894</td>\n",
              "      <td>0.030186</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015090</td>\n",
              "      <td>0.025890</td>\n",
              "      <td>0.042050</td>\n",
              "      <td>0.39600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concave points error</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.011796</td>\n",
              "      <td>0.006170</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007638</td>\n",
              "      <td>0.010930</td>\n",
              "      <td>0.014710</td>\n",
              "      <td>0.05279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>symmetry error</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.020542</td>\n",
              "      <td>0.008266</td>\n",
              "      <td>0.007882</td>\n",
              "      <td>0.015160</td>\n",
              "      <td>0.018730</td>\n",
              "      <td>0.023480</td>\n",
              "      <td>0.07895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fractal dimension error</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.003795</td>\n",
              "      <td>0.002646</td>\n",
              "      <td>0.000895</td>\n",
              "      <td>0.002248</td>\n",
              "      <td>0.003187</td>\n",
              "      <td>0.004558</td>\n",
              "      <td>0.02984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst radius</th>\n",
              "      <td>569.0</td>\n",
              "      <td>16.269190</td>\n",
              "      <td>4.833242</td>\n",
              "      <td>7.930000</td>\n",
              "      <td>13.010000</td>\n",
              "      <td>14.970000</td>\n",
              "      <td>18.790000</td>\n",
              "      <td>36.04000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst texture</th>\n",
              "      <td>569.0</td>\n",
              "      <td>25.677223</td>\n",
              "      <td>6.146258</td>\n",
              "      <td>12.020000</td>\n",
              "      <td>21.080000</td>\n",
              "      <td>25.410000</td>\n",
              "      <td>29.720000</td>\n",
              "      <td>49.54000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst perimeter</th>\n",
              "      <td>569.0</td>\n",
              "      <td>107.261213</td>\n",
              "      <td>33.602542</td>\n",
              "      <td>50.410000</td>\n",
              "      <td>84.110000</td>\n",
              "      <td>97.660000</td>\n",
              "      <td>125.400000</td>\n",
              "      <td>251.20000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst area</th>\n",
              "      <td>569.0</td>\n",
              "      <td>880.583128</td>\n",
              "      <td>569.356993</td>\n",
              "      <td>185.200000</td>\n",
              "      <td>515.300000</td>\n",
              "      <td>686.500000</td>\n",
              "      <td>1084.000000</td>\n",
              "      <td>4254.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst smoothness</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.132369</td>\n",
              "      <td>0.022832</td>\n",
              "      <td>0.071170</td>\n",
              "      <td>0.116600</td>\n",
              "      <td>0.131300</td>\n",
              "      <td>0.146000</td>\n",
              "      <td>0.22260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst compactness</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.254265</td>\n",
              "      <td>0.157336</td>\n",
              "      <td>0.027290</td>\n",
              "      <td>0.147200</td>\n",
              "      <td>0.211900</td>\n",
              "      <td>0.339100</td>\n",
              "      <td>1.05800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst concavity</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.272188</td>\n",
              "      <td>0.208624</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.114500</td>\n",
              "      <td>0.226700</td>\n",
              "      <td>0.382900</td>\n",
              "      <td>1.25200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst concave points</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.114606</td>\n",
              "      <td>0.065732</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.064930</td>\n",
              "      <td>0.099930</td>\n",
              "      <td>0.161400</td>\n",
              "      <td>0.29100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst symmetry</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.290076</td>\n",
              "      <td>0.061867</td>\n",
              "      <td>0.156500</td>\n",
              "      <td>0.250400</td>\n",
              "      <td>0.282200</td>\n",
              "      <td>0.317900</td>\n",
              "      <td>0.66380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst fractal dimension</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.083946</td>\n",
              "      <td>0.018061</td>\n",
              "      <td>0.055040</td>\n",
              "      <td>0.071460</td>\n",
              "      <td>0.080040</td>\n",
              "      <td>0.092080</td>\n",
              "      <td>0.20750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>benign_0__mal_1</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.627417</td>\n",
              "      <td>0.483918</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d559050b-1ec0-4c81-bf6e-10abc9724570')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d559050b-1ec0-4c81-bf6e-10abc9724570 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d559050b-1ec0-4c81-bf6e-10abc9724570');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                         count        mean         std         min  \\\n",
              "mean radius              569.0   14.127292    3.524049    6.981000   \n",
              "mean texture             569.0   19.289649    4.301036    9.710000   \n",
              "mean perimeter           569.0   91.969033   24.298981   43.790000   \n",
              "mean area                569.0  654.889104  351.914129  143.500000   \n",
              "mean smoothness          569.0    0.096360    0.014064    0.052630   \n",
              "mean compactness         569.0    0.104341    0.052813    0.019380   \n",
              "mean concavity           569.0    0.088799    0.079720    0.000000   \n",
              "mean concave points      569.0    0.048919    0.038803    0.000000   \n",
              "mean symmetry            569.0    0.181162    0.027414    0.106000   \n",
              "mean fractal dimension   569.0    0.062798    0.007060    0.049960   \n",
              "radius error             569.0    0.405172    0.277313    0.111500   \n",
              "texture error            569.0    1.216853    0.551648    0.360200   \n",
              "perimeter error          569.0    2.866059    2.021855    0.757000   \n",
              "area error               569.0   40.337079   45.491006    6.802000   \n",
              "smoothness error         569.0    0.007041    0.003003    0.001713   \n",
              "compactness error        569.0    0.025478    0.017908    0.002252   \n",
              "concavity error          569.0    0.031894    0.030186    0.000000   \n",
              "concave points error     569.0    0.011796    0.006170    0.000000   \n",
              "symmetry error           569.0    0.020542    0.008266    0.007882   \n",
              "fractal dimension error  569.0    0.003795    0.002646    0.000895   \n",
              "worst radius             569.0   16.269190    4.833242    7.930000   \n",
              "worst texture            569.0   25.677223    6.146258   12.020000   \n",
              "worst perimeter          569.0  107.261213   33.602542   50.410000   \n",
              "worst area               569.0  880.583128  569.356993  185.200000   \n",
              "worst smoothness         569.0    0.132369    0.022832    0.071170   \n",
              "worst compactness        569.0    0.254265    0.157336    0.027290   \n",
              "worst concavity          569.0    0.272188    0.208624    0.000000   \n",
              "worst concave points     569.0    0.114606    0.065732    0.000000   \n",
              "worst symmetry           569.0    0.290076    0.061867    0.156500   \n",
              "worst fractal dimension  569.0    0.083946    0.018061    0.055040   \n",
              "benign_0__mal_1          569.0    0.627417    0.483918    0.000000   \n",
              "\n",
              "                                25%         50%          75%         max  \n",
              "mean radius               11.700000   13.370000    15.780000    28.11000  \n",
              "mean texture              16.170000   18.840000    21.800000    39.28000  \n",
              "mean perimeter            75.170000   86.240000   104.100000   188.50000  \n",
              "mean area                420.300000  551.100000   782.700000  2501.00000  \n",
              "mean smoothness            0.086370    0.095870     0.105300     0.16340  \n",
              "mean compactness           0.064920    0.092630     0.130400     0.34540  \n",
              "mean concavity             0.029560    0.061540     0.130700     0.42680  \n",
              "mean concave points        0.020310    0.033500     0.074000     0.20120  \n",
              "mean symmetry              0.161900    0.179200     0.195700     0.30400  \n",
              "mean fractal dimension     0.057700    0.061540     0.066120     0.09744  \n",
              "radius error               0.232400    0.324200     0.478900     2.87300  \n",
              "texture error              0.833900    1.108000     1.474000     4.88500  \n",
              "perimeter error            1.606000    2.287000     3.357000    21.98000  \n",
              "area error                17.850000   24.530000    45.190000   542.20000  \n",
              "smoothness error           0.005169    0.006380     0.008146     0.03113  \n",
              "compactness error          0.013080    0.020450     0.032450     0.13540  \n",
              "concavity error            0.015090    0.025890     0.042050     0.39600  \n",
              "concave points error       0.007638    0.010930     0.014710     0.05279  \n",
              "symmetry error             0.015160    0.018730     0.023480     0.07895  \n",
              "fractal dimension error    0.002248    0.003187     0.004558     0.02984  \n",
              "worst radius              13.010000   14.970000    18.790000    36.04000  \n",
              "worst texture             21.080000   25.410000    29.720000    49.54000  \n",
              "worst perimeter           84.110000   97.660000   125.400000   251.20000  \n",
              "worst area               515.300000  686.500000  1084.000000  4254.00000  \n",
              "worst smoothness           0.116600    0.131300     0.146000     0.22260  \n",
              "worst compactness          0.147200    0.211900     0.339100     1.05800  \n",
              "worst concavity            0.114500    0.226700     0.382900     1.25200  \n",
              "worst concave points       0.064930    0.099930     0.161400     0.29100  \n",
              "worst symmetry             0.250400    0.282200     0.317900     0.66380  \n",
              "worst fractal dimension    0.071460    0.080040     0.092080     0.20750  \n",
              "benign_0__mal_1            0.000000    1.000000     1.000000     1.00000  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.describe().transpose() #displaying transpose (rows as columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg-R1HfDAHuG"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgCbsPppAHuH"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns #importing required libraries\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuwGPWKrAHuH"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x='benign_0__mal_1',data=df) #constructing a countplot between cancer type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHaafVQWAHuI"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(df.corr()) #constructing a correlational heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KmfweUNxAHuI",
        "outputId": "6f19907c-d547-479f-bedf-0a97669c08eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "worst concave points      -0.793566\n",
              "worst perimeter           -0.782914\n",
              "mean concave points       -0.776614\n",
              "worst radius              -0.776454\n",
              "mean perimeter            -0.742636\n",
              "worst area                -0.733825\n",
              "mean radius               -0.730029\n",
              "mean area                 -0.708984\n",
              "mean concavity            -0.696360\n",
              "worst concavity           -0.659610\n",
              "mean compactness          -0.596534\n",
              "worst compactness         -0.590998\n",
              "radius error              -0.567134\n",
              "perimeter error           -0.556141\n",
              "area error                -0.548236\n",
              "worst texture             -0.456903\n",
              "worst smoothness          -0.421465\n",
              "worst symmetry            -0.416294\n",
              "mean texture              -0.415185\n",
              "concave points error      -0.408042\n",
              "mean smoothness           -0.358560\n",
              "mean symmetry             -0.330499\n",
              "worst fractal dimension   -0.323872\n",
              "compactness error         -0.292999\n",
              "concavity error           -0.253730\n",
              "fractal dimension error   -0.077972\n",
              "symmetry error             0.006522\n",
              "texture error              0.008303\n",
              "mean fractal dimension     0.012838\n",
              "smoothness error           0.067016\n",
              "benign_0__mal_1            1.000000\n",
              "Name: benign_0__mal_1, dtype: float64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.corr()['benign_0__mal_1'].sort_values() #sorting the dataset wrt benign_0__mal_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIxDnpt8AHuI"
      },
      "outputs": [],
      "source": [
        "df.corr()['benign_0__mal_1'].sort_values().plot(kind='bar') #plotting the sorted values via bars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g42UoTj-AHuJ"
      },
      "outputs": [],
      "source": [
        "df.corr()['benign_0__mal_1'][:-1].sort_values().plot(kind='bar') #shifting the axis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qi60FPxYAHuJ"
      },
      "source": [
        "## Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81JZh2E6AHuJ"
      },
      "outputs": [],
      "source": [
        "X = df.drop('benign_0__mal_1',axis=1).values #training the x, axis1 = removing a parameter\n",
        "y = df['benign_0__mal_1'].values  #training the y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e13l4vYiAHuJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split #impoting the required function from the library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "_n6lYocMAHuK",
        "outputId": "9c7755a6-f9ff-45bc-9b00-208c42fe31b3"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b97d5f3e8fee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=101)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNY9u1_lAHuK"
      },
      "source": [
        "\n",
        "## Scaling Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOgKpCkfAHuK"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler #impoting the required function from the library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcnsJ_sGAHuK"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler() #(subtracts the minimum value in the feature and then divides by the range)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cv-UxUnkAHuK",
        "outputId": "ccc5944f-9cb9-4e68-ed6e-bc203855f317"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MinMaxScaler()"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scaler.fit(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjk6_JJTAHuK"
      },
      "outputs": [],
      "source": [
        "X_train = scaler.transform(X_train) #transforming the data\n",
        "X_test = scaler.transform(X_test)  #transforming the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l8ak0LTAHuK"
      },
      "source": [
        "## Creating the Model\n",
        "\n",
        "    # For a binary classification problem\n",
        "    model.compile(optimizer='rmsprop',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "                  \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgKhnDTaAHuL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf  #importing required library\n",
        "from tensorflow.keras.models import Sequential #impoting the required function from the library\n",
        "from tensorflow.keras.layers import Dense, Activation,Dropout  #impoting the required function from the library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3O68gJAwAHuL",
        "outputId": "86ac6ceb-f919-4e60-807e-5056a5f54795"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(426, 30)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape #shaping the X data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGuX48_iAHuL"
      },
      "outputs": [],
      "source": [
        "model = Sequential() #making a sequential type model\n",
        "\n",
        "# https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
        "\n",
        "model.add(Dense(units=30,activation='relu')) #creating a layer using 30 neurons and relu function (relu is used as it gives least amount of loss, and helps to prevent the exponential growth)\n",
        "#defines the size of the output from the dense layer(represents the dimensionality of the output vector)\n",
        "model.add(Dense(units=15,activation='relu')) #creating a layer using 15 neurons and relu function (relu is used as it gives least amount of loss, and helps to prevent the exponential growth)\n",
        "\n",
        "model.add(Dense(units=1,activation='sigmoid')) #creating a layer using 1 neurons and sigmoid function (because it exists between (0 to 1), it is mailnly used at placed where we expect probability like answers)\n",
        "\n",
        "\n",
        "# For a binary classification problem\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam') #compiling our model using adam optimizer (results of the Adam optimizer are generally better than every other optimization algorithms, have faster computation time, and require fewer parameters for tuning)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Olx5erTgAHuL"
      },
      "source": [
        "## Training the Model\n",
        "\n",
        "### Example One: Choosing too many epochs and overfitting!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeLRFal_AHuL",
        "outputId": "9fdc1824-fd4e-4bbe-863a-a50112daa163",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0091 - val_loss: 0.3238\n",
            "Epoch 2/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0054 - val_loss: 0.2900\n",
            "Epoch 3/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0087 - val_loss: 0.3012\n",
            "Epoch 4/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0074 - val_loss: 0.3117\n",
            "Epoch 5/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0084 - val_loss: 0.3292\n",
            "Epoch 6/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0062 - val_loss: 0.3042\n",
            "Epoch 7/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0054 - val_loss: 0.3156\n",
            "Epoch 8/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0056 - val_loss: 0.3029\n",
            "Epoch 9/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0050 - val_loss: 0.3260\n",
            "Epoch 10/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0059 - val_loss: 0.3175\n",
            "Epoch 11/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0050 - val_loss: 0.3044\n",
            "Epoch 12/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0068 - val_loss: 0.3305\n",
            "Epoch 13/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0058 - val_loss: 0.3147\n",
            "Epoch 14/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0051 - val_loss: 0.3198\n",
            "Epoch 15/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0050 - val_loss: 0.3156\n",
            "Epoch 16/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0089 - val_loss: 0.3181\n",
            "Epoch 17/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0079 - val_loss: 0.3542\n",
            "Epoch 18/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.3057\n",
            "Epoch 19/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0082 - val_loss: 0.3063\n",
            "Epoch 20/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0073 - val_loss: 0.3278\n",
            "Epoch 21/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0056 - val_loss: 0.3135\n",
            "Epoch 22/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0045 - val_loss: 0.3157\n",
            "Epoch 23/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0043 - val_loss: 0.3207\n",
            "Epoch 24/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0044 - val_loss: 0.3229\n",
            "Epoch 25/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0052 - val_loss: 0.3365\n",
            "Epoch 26/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0050 - val_loss: 0.3261\n",
            "Epoch 27/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0064 - val_loss: 0.3170\n",
            "Epoch 28/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0055 - val_loss: 0.3411\n",
            "Epoch 29/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0058 - val_loss: 0.3267\n",
            "Epoch 30/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0046 - val_loss: 0.3347\n",
            "Epoch 31/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0041 - val_loss: 0.3352\n",
            "Epoch 32/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0039 - val_loss: 0.3331\n",
            "Epoch 33/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0039 - val_loss: 0.3312\n",
            "Epoch 34/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0042 - val_loss: 0.3334\n",
            "Epoch 35/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0048 - val_loss: 0.3294\n",
            "Epoch 36/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0041 - val_loss: 0.3370\n",
            "Epoch 37/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0044 - val_loss: 0.3378\n",
            "Epoch 38/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0044 - val_loss: 0.3400\n",
            "Epoch 39/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0043 - val_loss: 0.3411\n",
            "Epoch 40/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0038 - val_loss: 0.3321\n",
            "Epoch 41/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0061 - val_loss: 0.3680\n",
            "Epoch 42/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0045 - val_loss: 0.3537\n",
            "Epoch 43/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0040 - val_loss: 0.3449\n",
            "Epoch 44/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0046 - val_loss: 0.3382\n",
            "Epoch 45/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0041 - val_loss: 0.3534\n",
            "Epoch 46/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0040 - val_loss: 0.3454\n",
            "Epoch 47/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0042 - val_loss: 0.3507\n",
            "Epoch 48/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0038 - val_loss: 0.3440\n",
            "Epoch 49/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0043 - val_loss: 0.3636\n",
            "Epoch 50/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0037 - val_loss: 0.3605\n",
            "Epoch 51/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0035 - val_loss: 0.3588\n",
            "Epoch 52/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0034 - val_loss: 0.3374\n",
            "Epoch 53/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0042 - val_loss: 0.3638\n",
            "Epoch 54/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0038 - val_loss: 0.3627\n",
            "Epoch 55/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0034 - val_loss: 0.3635\n",
            "Epoch 56/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0044 - val_loss: 0.3514\n",
            "Epoch 57/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0047 - val_loss: 0.3760\n",
            "Epoch 58/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.3641\n",
            "Epoch 59/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0034 - val_loss: 0.3702\n",
            "Epoch 60/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.3646\n",
            "Epoch 61/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0044 - val_loss: 0.3734\n",
            "Epoch 62/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0039 - val_loss: 0.3831\n",
            "Epoch 63/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0040 - val_loss: 0.3631\n",
            "Epoch 64/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0053 - val_loss: 0.3858\n",
            "Epoch 65/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0037 - val_loss: 0.3786\n",
            "Epoch 66/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0032 - val_loss: 0.3737\n",
            "Epoch 67/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.3767\n",
            "Epoch 68/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0033 - val_loss: 0.3845\n",
            "Epoch 69/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.3741\n",
            "Epoch 70/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0034 - val_loss: 0.3827\n",
            "Epoch 71/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.3820\n",
            "Epoch 72/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0030 - val_loss: 0.3804\n",
            "Epoch 73/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0029 - val_loss: 0.3819\n",
            "Epoch 74/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0030 - val_loss: 0.3852\n",
            "Epoch 75/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0050 - val_loss: 0.3633\n",
            "Epoch 76/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0048 - val_loss: 0.4124\n",
            "Epoch 77/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0049 - val_loss: 0.3851\n",
            "Epoch 78/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0038 - val_loss: 0.3816\n",
            "Epoch 79/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0032 - val_loss: 0.3793\n",
            "Epoch 80/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0031 - val_loss: 0.3888\n",
            "Epoch 81/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0031 - val_loss: 0.3762\n",
            "Epoch 82/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0028 - val_loss: 0.3947\n",
            "Epoch 83/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0031 - val_loss: 0.3968\n",
            "Epoch 84/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0036 - val_loss: 0.3845\n",
            "Epoch 85/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0029 - val_loss: 0.3981\n",
            "Epoch 86/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0039 - val_loss: 0.3882\n",
            "Epoch 87/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0044 - val_loss: 0.3926\n",
            "Epoch 88/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0027 - val_loss: 0.3838\n",
            "Epoch 89/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0027 - val_loss: 0.4023\n",
            "Epoch 90/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0026 - val_loss: 0.4016\n",
            "Epoch 91/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0030 - val_loss: 0.4151\n",
            "Epoch 92/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.3919\n",
            "Epoch 93/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0037 - val_loss: 0.3894\n",
            "Epoch 94/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.4063\n",
            "Epoch 95/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0031 - val_loss: 0.3999\n",
            "Epoch 96/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0025 - val_loss: 0.3999\n",
            "Epoch 97/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0025 - val_loss: 0.4072\n",
            "Epoch 98/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0025 - val_loss: 0.4168\n",
            "Epoch 99/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0024 - val_loss: 0.4054\n",
            "Epoch 100/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0033 - val_loss: 0.4209\n",
            "Epoch 101/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0027 - val_loss: 0.4012\n",
            "Epoch 102/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0026 - val_loss: 0.4154\n",
            "Epoch 103/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0023 - val_loss: 0.4073\n",
            "Epoch 104/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0028 - val_loss: 0.4318\n",
            "Epoch 105/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0029 - val_loss: 0.4143\n",
            "Epoch 106/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0030 - val_loss: 0.4211\n",
            "Epoch 107/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.4391\n",
            "Epoch 108/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.4152\n",
            "Epoch 109/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0034 - val_loss: 0.4299\n",
            "Epoch 110/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0024 - val_loss: 0.4130\n",
            "Epoch 111/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0025 - val_loss: 0.4314\n",
            "Epoch 112/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0027 - val_loss: 0.4198\n",
            "Epoch 113/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0024 - val_loss: 0.4312\n",
            "Epoch 114/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0028 - val_loss: 0.4191\n",
            "Epoch 115/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0032 - val_loss: 0.4355\n",
            "Epoch 116/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0034 - val_loss: 0.4195\n",
            "Epoch 117/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0031 - val_loss: 0.4461\n",
            "Epoch 118/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0025 - val_loss: 0.4266\n",
            "Epoch 119/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.4375\n",
            "Epoch 120/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0027 - val_loss: 0.4267\n",
            "Epoch 121/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0026 - val_loss: 0.4366\n",
            "Epoch 122/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0021 - val_loss: 0.4376\n",
            "Epoch 123/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0021 - val_loss: 0.4335\n",
            "Epoch 124/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0023 - val_loss: 0.4327\n",
            "Epoch 125/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0022 - val_loss: 0.4450\n",
            "Epoch 126/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0024 - val_loss: 0.4474\n",
            "Epoch 127/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0021 - val_loss: 0.4446\n",
            "Epoch 128/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0019 - val_loss: 0.4472\n",
            "Epoch 129/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0020 - val_loss: 0.4402\n",
            "Epoch 130/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0023 - val_loss: 0.4647\n",
            "Epoch 131/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0022 - val_loss: 0.4465\n",
            "Epoch 132/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0021 - val_loss: 0.4452\n",
            "Epoch 133/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0019 - val_loss: 0.4502\n",
            "Epoch 134/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0019 - val_loss: 0.4480\n",
            "Epoch 135/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0024 - val_loss: 0.4719\n",
            "Epoch 136/600\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0023 - val_loss: 0.4402\n",
            "Epoch 137/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0025 - val_loss: 0.4521\n",
            "Epoch 138/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0018 - val_loss: 0.4475\n",
            "Epoch 139/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0017 - val_loss: 0.4634\n",
            "Epoch 140/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0020 - val_loss: 0.4536\n",
            "Epoch 141/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0018 - val_loss: 0.4550\n",
            "Epoch 142/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0018 - val_loss: 0.4656\n",
            "Epoch 143/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0019 - val_loss: 0.4573\n",
            "Epoch 144/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0023 - val_loss: 0.4498\n",
            "Epoch 145/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0021 - val_loss: 0.4747\n",
            "Epoch 146/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0026 - val_loss: 0.4634\n",
            "Epoch 147/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0023 - val_loss: 0.4750\n",
            "Epoch 148/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0021 - val_loss: 0.4589\n",
            "Epoch 149/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0021 - val_loss: 0.4533\n",
            "Epoch 150/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0018 - val_loss: 0.4679\n",
            "Epoch 151/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 0.4639\n",
            "Epoch 152/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0017 - val_loss: 0.4694\n",
            "Epoch 153/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0019 - val_loss: 0.4628\n",
            "Epoch 154/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0017 - val_loss: 0.4663\n",
            "Epoch 155/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0016 - val_loss: 0.4680\n",
            "Epoch 156/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0020 - val_loss: 0.4797\n",
            "Epoch 157/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0021 - val_loss: 0.4732\n",
            "Epoch 158/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0018 - val_loss: 0.4838\n",
            "Epoch 159/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0021 - val_loss: 0.4665\n",
            "Epoch 160/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0021 - val_loss: 0.4667\n",
            "Epoch 161/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.4723\n",
            "Epoch 162/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0016 - val_loss: 0.4716\n",
            "Epoch 163/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0018 - val_loss: 0.4776\n",
            "Epoch 164/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0017 - val_loss: 0.4777\n",
            "Epoch 165/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0015 - val_loss: 0.4852\n",
            "Epoch 166/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.4823\n",
            "Epoch 167/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0017 - val_loss: 0.4825\n",
            "Epoch 168/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0023 - val_loss: 0.4994\n",
            "Epoch 169/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0021 - val_loss: 0.4799\n",
            "Epoch 170/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0014 - val_loss: 0.4905\n",
            "Epoch 171/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0015 - val_loss: 0.4842\n",
            "Epoch 172/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 0.4891\n",
            "Epoch 173/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0016 - val_loss: 0.5020\n",
            "Epoch 174/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0016 - val_loss: 0.4875\n",
            "Epoch 175/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0019 - val_loss: 0.5025\n",
            "Epoch 176/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0019 - val_loss: 0.4778\n",
            "Epoch 177/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0016 - val_loss: 0.4925\n",
            "Epoch 178/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0016 - val_loss: 0.4859\n",
            "Epoch 179/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0015 - val_loss: 0.5071\n",
            "Epoch 180/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0013 - val_loss: 0.4897\n",
            "Epoch 181/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0014 - val_loss: 0.5035\n",
            "Epoch 182/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0021 - val_loss: 0.4919\n",
            "Epoch 183/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0013 - val_loss: 0.4955\n",
            "Epoch 184/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0014 - val_loss: 0.4884\n",
            "Epoch 185/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0014 - val_loss: 0.4853\n",
            "Epoch 186/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 0.4989\n",
            "Epoch 187/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0012 - val_loss: 0.4948\n",
            "Epoch 188/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0013 - val_loss: 0.4913\n",
            "Epoch 189/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0015 - val_loss: 0.5086\n",
            "Epoch 190/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0013 - val_loss: 0.5047\n",
            "Epoch 191/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0014 - val_loss: 0.5091\n",
            "Epoch 192/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 0.5057\n",
            "Epoch 193/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 0.5058\n",
            "Epoch 194/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0014 - val_loss: 0.5079\n",
            "Epoch 195/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0016 - val_loss: 0.5058\n",
            "Epoch 196/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0013 - val_loss: 0.5041\n",
            "Epoch 197/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 0.5032\n",
            "Epoch 198/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0012 - val_loss: 0.5094\n",
            "Epoch 199/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0012 - val_loss: 0.5108\n",
            "Epoch 200/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 0.5055\n",
            "Epoch 201/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0013 - val_loss: 0.5155\n",
            "Epoch 202/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0013 - val_loss: 0.5215\n",
            "Epoch 203/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0013 - val_loss: 0.5132\n",
            "Epoch 204/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0012 - val_loss: 0.5022\n",
            "Epoch 205/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0012 - val_loss: 0.5240\n",
            "Epoch 206/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0021 - val_loss: 0.4988\n",
            "Epoch 207/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0012 - val_loss: 0.5296\n",
            "Epoch 208/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 0.5063\n",
            "Epoch 209/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0014 - val_loss: 0.5168\n",
            "Epoch 210/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0010 - val_loss: 0.5001\n",
            "Epoch 211/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0011 - val_loss: 0.5149\n",
            "Epoch 212/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0010 - val_loss: 0.5207\n",
            "Epoch 213/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0010 - val_loss: 0.5213\n",
            "Epoch 214/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 0.5259\n",
            "Epoch 215/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0011 - val_loss: 0.5244\n",
            "Epoch 216/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 9.9701e-04 - val_loss: 0.5233\n",
            "Epoch 217/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 9.8691e-04 - val_loss: 0.5278\n",
            "Epoch 218/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 0.5365\n",
            "Epoch 219/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0011 - val_loss: 0.5305\n",
            "Epoch 220/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0019 - val_loss: 0.5763\n",
            "Epoch 221/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0025 - val_loss: 0.5259\n",
            "Epoch 222/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0015 - val_loss: 0.5677\n",
            "Epoch 223/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 0.5409\n",
            "Epoch 224/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 0.5197\n",
            "Epoch 225/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 0.5368\n",
            "Epoch 226/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 0.5358\n",
            "Epoch 227/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0010 - val_loss: 0.5373\n",
            "Epoch 228/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 9.6435e-04 - val_loss: 0.5482\n",
            "Epoch 229/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 0.5345\n",
            "Epoch 230/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 0.5508\n",
            "Epoch 231/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0014 - val_loss: 0.5655\n",
            "Epoch 232/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 0.5278\n",
            "Epoch 233/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0014 - val_loss: 0.5546\n",
            "Epoch 234/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 8.8547e-04 - val_loss: 0.5392\n",
            "Epoch 235/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 9.4868e-04 - val_loss: 0.5491\n",
            "Epoch 236/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 8.2090e-04 - val_loss: 0.5407\n",
            "Epoch 237/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 8.0555e-04 - val_loss: 0.5543\n",
            "Epoch 238/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 9.8297e-04 - val_loss: 0.5493\n",
            "Epoch 239/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 8.1958e-04 - val_loss: 0.5544\n",
            "Epoch 240/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 0.5448\n",
            "Epoch 241/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0018 - val_loss: 0.5936\n",
            "Epoch 242/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0021 - val_loss: 0.6134\n",
            "Epoch 243/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0049 - val_loss: 0.5545\n",
            "Epoch 244/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 0.5814\n",
            "Epoch 245/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0011 - val_loss: 0.5561\n",
            "Epoch 246/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 8.5964e-04 - val_loss: 0.5518\n",
            "Epoch 247/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 9.5071e-04 - val_loss: 0.5524\n",
            "Epoch 248/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 8.6217e-04 - val_loss: 0.5646\n",
            "Epoch 249/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 9.8988e-04 - val_loss: 0.5644\n",
            "Epoch 250/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 7.8321e-04 - val_loss: 0.5590\n",
            "Epoch 251/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 7.8073e-04 - val_loss: 0.5554\n",
            "Epoch 252/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 8.1896e-04 - val_loss: 0.5628\n",
            "Epoch 253/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 9.0218e-04 - val_loss: 0.5785\n",
            "Epoch 254/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 9.2412e-04 - val_loss: 0.5626\n",
            "Epoch 255/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 7.3579e-04 - val_loss: 0.5729\n",
            "Epoch 256/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 7.9749e-04 - val_loss: 0.5709\n",
            "Epoch 257/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 0.5383\n",
            "Epoch 258/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 0.5746\n",
            "Epoch 259/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 8.7028e-04 - val_loss: 0.5915\n",
            "Epoch 260/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 8.3746e-04 - val_loss: 0.5798\n",
            "Epoch 261/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 8.0125e-04 - val_loss: 0.5842\n",
            "Epoch 262/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 7.3303e-04 - val_loss: 0.5797\n",
            "Epoch 263/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 7.1058e-04 - val_loss: 0.5775\n",
            "Epoch 264/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 7.8504e-04 - val_loss: 0.5887\n",
            "Epoch 265/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 7.0523e-04 - val_loss: 0.5773\n",
            "Epoch 266/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 6.8584e-04 - val_loss: 0.5819\n",
            "Epoch 267/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 8.6938e-04 - val_loss: 0.5714\n",
            "Epoch 268/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0015 - val_loss: 0.6300\n",
            "Epoch 269/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0181 - val_loss: 0.6848\n",
            "Epoch 270/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0056 - val_loss: 0.6041\n",
            "Epoch 271/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0033 - val_loss: 0.6233\n",
            "Epoch 272/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0031 - val_loss: 0.6619\n",
            "Epoch 273/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0025 - val_loss: 0.6203\n",
            "Epoch 274/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 8.7333e-04 - val_loss: 0.6395\n",
            "Epoch 275/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 8.9259e-04 - val_loss: 0.6187\n",
            "Epoch 276/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 8.1115e-04 - val_loss: 0.6190\n",
            "Epoch 277/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 7.7674e-04 - val_loss: 0.6181\n",
            "Epoch 278/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 7.3670e-04 - val_loss: 0.6111\n",
            "Epoch 279/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 6.4364e-04 - val_loss: 0.6066\n",
            "Epoch 280/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 6.8917e-04 - val_loss: 0.6049\n",
            "Epoch 281/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 7.2315e-04 - val_loss: 0.6044\n",
            "Epoch 282/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 9.7675e-04 - val_loss: 0.6076\n",
            "Epoch 283/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 7.4562e-04 - val_loss: 0.6069\n",
            "Epoch 284/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 6.2532e-04 - val_loss: 0.6040\n",
            "Epoch 285/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 5.9951e-04 - val_loss: 0.6013\n",
            "Epoch 286/600\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 6.3406e-04 - val_loss: 0.6009\n",
            "Epoch 287/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 6.0193e-04 - val_loss: 0.6070\n",
            "Epoch 288/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 6.5496e-04 - val_loss: 0.6069\n",
            "Epoch 289/600\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 6.5581e-04 - val_loss: 0.6060\n",
            "Epoch 290/600\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 5.6894e-04 - val_loss: 0.6070\n",
            "Epoch 291/600\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 6.2736e-04 - val_loss: 0.6009\n",
            "Epoch 292/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 5.9696e-04 - val_loss: 0.6097\n",
            "Epoch 293/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 5.8613e-04 - val_loss: 0.6128\n",
            "Epoch 294/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 6.6005e-04 - val_loss: 0.6116\n",
            "Epoch 295/600\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 5.4682e-04 - val_loss: 0.6056\n",
            "Epoch 296/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 5.8836e-04 - val_loss: 0.6171\n",
            "Epoch 297/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 5.7924e-04 - val_loss: 0.6112\n",
            "Epoch 298/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 6.0181e-04 - val_loss: 0.6185\n",
            "Epoch 299/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 5.8774e-04 - val_loss: 0.6157\n",
            "Epoch 300/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 5.4274e-04 - val_loss: 0.6153\n",
            "Epoch 301/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 5.4393e-04 - val_loss: 0.6122\n",
            "Epoch 302/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 5.4957e-04 - val_loss: 0.6090\n",
            "Epoch 303/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 5.4328e-04 - val_loss: 0.6138\n",
            "Epoch 304/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 6.3687e-04 - val_loss: 0.6174\n",
            "Epoch 305/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 9.6541e-04 - val_loss: 0.6227\n",
            "Epoch 306/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 6.7085e-04 - val_loss: 0.6170\n",
            "Epoch 307/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 6.4162e-04 - val_loss: 0.6251\n",
            "Epoch 308/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 5.5763e-04 - val_loss: 0.6153\n",
            "Epoch 309/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 5.2188e-04 - val_loss: 0.6250\n",
            "Epoch 310/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 5.7210e-04 - val_loss: 0.6240\n",
            "Epoch 311/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 5.2399e-04 - val_loss: 0.6243\n",
            "Epoch 312/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 5.1065e-04 - val_loss: 0.6217\n",
            "Epoch 313/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 5.0508e-04 - val_loss: 0.6243\n",
            "Epoch 314/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 5.0808e-04 - val_loss: 0.6265\n",
            "Epoch 315/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 5.1734e-04 - val_loss: 0.6262\n",
            "Epoch 316/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 5.4159e-04 - val_loss: 0.6224\n",
            "Epoch 317/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 5.0130e-04 - val_loss: 0.6245\n",
            "Epoch 318/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 5.0292e-04 - val_loss: 0.6207\n",
            "Epoch 319/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 5.1566e-04 - val_loss: 0.6275\n",
            "Epoch 320/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 5.0959e-04 - val_loss: 0.6265\n",
            "Epoch 321/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 5.1462e-04 - val_loss: 0.6309\n",
            "Epoch 322/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 4.8300e-04 - val_loss: 0.6271\n",
            "Epoch 323/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 5.7293e-04 - val_loss: 0.6362\n",
            "Epoch 324/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 4.9929e-04 - val_loss: 0.6332\n",
            "Epoch 325/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 4.8952e-04 - val_loss: 0.6301\n",
            "Epoch 326/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 4.9504e-04 - val_loss: 0.6359\n",
            "Epoch 327/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 4.8003e-04 - val_loss: 0.6311\n",
            "Epoch 328/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 4.7043e-04 - val_loss: 0.6367\n",
            "Epoch 329/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 4.7246e-04 - val_loss: 0.6333\n",
            "Epoch 330/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 4.5571e-04 - val_loss: 0.6361\n",
            "Epoch 331/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 4.6900e-04 - val_loss: 0.6379\n",
            "Epoch 332/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 5.9697e-04 - val_loss: 0.6423\n",
            "Epoch 333/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 8.3233e-04 - val_loss: 0.6428\n",
            "Epoch 334/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 9.3258e-04 - val_loss: 0.6446\n",
            "Epoch 335/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 8.0326e-04 - val_loss: 0.6277\n",
            "Epoch 336/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 4.8913e-04 - val_loss: 0.6251\n",
            "Epoch 337/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 4.5265e-04 - val_loss: 0.6282\n",
            "Epoch 338/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 4.5069e-04 - val_loss: 0.6318\n",
            "Epoch 339/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 4.4294e-04 - val_loss: 0.6339\n",
            "Epoch 340/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 4.5205e-04 - val_loss: 0.6367\n",
            "Epoch 341/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 4.4372e-04 - val_loss: 0.6372\n",
            "Epoch 342/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 4.8511e-04 - val_loss: 0.6404\n",
            "Epoch 343/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 4.4342e-04 - val_loss: 0.6401\n",
            "Epoch 344/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 4.4769e-04 - val_loss: 0.6411\n",
            "Epoch 345/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 4.2150e-04 - val_loss: 0.6413\n",
            "Epoch 346/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 4.5211e-04 - val_loss: 0.6409\n",
            "Epoch 347/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 4.3220e-04 - val_loss: 0.6495\n",
            "Epoch 348/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 4.2350e-04 - val_loss: 0.6472\n",
            "Epoch 349/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 4.1573e-04 - val_loss: 0.6464\n",
            "Epoch 350/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 4.9422e-04 - val_loss: 0.6512\n",
            "Epoch 351/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 4.7411e-04 - val_loss: 0.6551\n",
            "Epoch 352/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 4.3386e-04 - val_loss: 0.6513\n",
            "Epoch 353/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 4.3185e-04 - val_loss: 0.6440\n",
            "Epoch 354/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 4.6164e-04 - val_loss: 0.6515\n",
            "Epoch 355/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 4.5607e-04 - val_loss: 0.6541\n",
            "Epoch 356/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 4.1750e-04 - val_loss: 0.6514\n",
            "Epoch 357/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 4.1822e-04 - val_loss: 0.6476\n",
            "Epoch 358/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 5.7110e-04 - val_loss: 0.6637\n",
            "Epoch 359/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 6.3851e-04 - val_loss: 0.6527\n",
            "Epoch 360/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 8.3171e-04 - val_loss: 0.6677\n",
            "Epoch 361/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0010 - val_loss: 0.6453\n",
            "Epoch 362/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 6.9203e-04 - val_loss: 0.6631\n",
            "Epoch 363/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 6.5302e-04 - val_loss: 0.6326\n",
            "Epoch 364/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 4.6923e-04 - val_loss: 0.6591\n",
            "Epoch 365/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 3.9995e-04 - val_loss: 0.6594\n",
            "Epoch 366/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 4.0294e-04 - val_loss: 0.6589\n",
            "Epoch 367/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 3.8191e-04 - val_loss: 0.6576\n",
            "Epoch 368/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 3.6917e-04 - val_loss: 0.6594\n",
            "Epoch 369/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 4.5223e-04 - val_loss: 0.6647\n",
            "Epoch 370/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 4.6659e-04 - val_loss: 0.6674\n",
            "Epoch 371/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 3.8992e-04 - val_loss: 0.6651\n",
            "Epoch 372/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 3.8213e-04 - val_loss: 0.6632\n",
            "Epoch 373/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 4.7248e-04 - val_loss: 0.6616\n",
            "Epoch 374/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 4.6873e-04 - val_loss: 0.6696\n",
            "Epoch 375/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 3.5465e-04 - val_loss: 0.6701\n",
            "Epoch 376/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 3.7945e-04 - val_loss: 0.6666\n",
            "Epoch 377/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 3.5647e-04 - val_loss: 0.6719\n",
            "Epoch 378/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 3.5927e-04 - val_loss: 0.6697\n",
            "Epoch 379/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 3.4462e-04 - val_loss: 0.6690\n",
            "Epoch 380/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 3.6362e-04 - val_loss: 0.6731\n",
            "Epoch 381/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 5.8974e-04 - val_loss: 0.6639\n",
            "Epoch 382/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 4.8606e-04 - val_loss: 0.6786\n",
            "Epoch 383/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 4.8338e-04 - val_loss: 0.6577\n",
            "Epoch 384/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 3.9763e-04 - val_loss: 0.6662\n",
            "Epoch 385/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 5.4943e-04 - val_loss: 0.6750\n",
            "Epoch 386/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 5.2421e-04 - val_loss: 0.6716\n",
            "Epoch 387/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 3.4806e-04 - val_loss: 0.6733\n",
            "Epoch 388/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 3.4041e-04 - val_loss: 0.6760\n",
            "Epoch 389/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 3.8787e-04 - val_loss: 0.6858\n",
            "Epoch 390/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 3.6584e-04 - val_loss: 0.6850\n",
            "Epoch 391/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 3.7407e-04 - val_loss: 0.6778\n",
            "Epoch 392/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 3.6108e-04 - val_loss: 0.6881\n",
            "Epoch 393/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 3.2892e-04 - val_loss: 0.6871\n",
            "Epoch 394/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 3.2717e-04 - val_loss: 0.6842\n",
            "Epoch 395/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 3.4176e-04 - val_loss: 0.6891\n",
            "Epoch 396/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 3.6216e-04 - val_loss: 0.6844\n",
            "Epoch 397/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 3.2952e-04 - val_loss: 0.6919\n",
            "Epoch 398/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 3.7211e-04 - val_loss: 0.6932\n",
            "Epoch 399/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 5.1231e-04 - val_loss: 0.6882\n",
            "Epoch 400/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 4.0894e-04 - val_loss: 0.6729\n",
            "Epoch 401/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 3.2985e-04 - val_loss: 0.6738\n",
            "Epoch 402/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 3.2862e-04 - val_loss: 0.6823\n",
            "Epoch 403/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 3.1583e-04 - val_loss: 0.6872\n",
            "Epoch 404/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 3.6329e-04 - val_loss: 0.6881\n",
            "Epoch 405/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 3.0323e-04 - val_loss: 0.6875\n",
            "Epoch 406/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 3.0058e-04 - val_loss: 0.6965\n",
            "Epoch 407/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 3.0788e-04 - val_loss: 0.6888\n",
            "Epoch 408/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 3.2568e-04 - val_loss: 0.6949\n",
            "Epoch 409/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 3.8863e-04 - val_loss: 0.6906\n",
            "Epoch 410/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 3.2864e-04 - val_loss: 0.6963\n",
            "Epoch 411/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 2.8223e-04 - val_loss: 0.6948\n",
            "Epoch 412/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 2.8923e-04 - val_loss: 0.6945\n",
            "Epoch 413/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 3.1899e-04 - val_loss: 0.7031\n",
            "Epoch 414/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 3.1638e-04 - val_loss: 0.6902\n",
            "Epoch 415/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 3.0011e-04 - val_loss: 0.6967\n",
            "Epoch 416/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 2.8692e-04 - val_loss: 0.7013\n",
            "Epoch 417/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 3.0305e-04 - val_loss: 0.6968\n",
            "Epoch 418/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.8837e-04 - val_loss: 0.7075\n",
            "Epoch 419/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 2.6596e-04 - val_loss: 0.7052\n",
            "Epoch 420/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.6957e-04 - val_loss: 0.7118\n",
            "Epoch 421/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.8550e-04 - val_loss: 0.7032\n",
            "Epoch 422/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.9078e-04 - val_loss: 0.7100\n",
            "Epoch 423/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.8101e-04 - val_loss: 0.7091\n",
            "Epoch 424/600\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 2.7850e-04 - val_loss: 0.7057\n",
            "Epoch 425/600\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 2.8162e-04 - val_loss: 0.7096\n",
            "Epoch 426/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 3.1055e-04 - val_loss: 0.7040\n",
            "Epoch 427/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 2.7062e-04 - val_loss: 0.7061\n",
            "Epoch 428/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 2.6933e-04 - val_loss: 0.7074\n",
            "Epoch 429/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 3.2260e-04 - val_loss: 0.7173\n",
            "Epoch 430/600\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 3.0555e-04 - val_loss: 0.6923\n",
            "Epoch 431/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 4.7295e-04 - val_loss: 0.7368\n",
            "Epoch 432/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0012 - val_loss: 0.7165\n",
            "Epoch 433/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0022 - val_loss: 0.7187\n",
            "Epoch 434/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0081 - val_loss: 0.6603\n",
            "Epoch 435/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0321 - val_loss: 0.5747\n",
            "Epoch 436/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0095 - val_loss: 0.6328\n",
            "Epoch 437/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0024 - val_loss: 0.6397\n",
            "Epoch 438/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 5.3934e-04 - val_loss: 0.6415\n",
            "Epoch 439/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 5.1038e-04 - val_loss: 0.6479\n",
            "Epoch 440/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 4.0342e-04 - val_loss: 0.6520\n",
            "Epoch 441/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 3.7968e-04 - val_loss: 0.6546\n",
            "Epoch 442/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 3.5588e-04 - val_loss: 0.6595\n",
            "Epoch 443/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 3.5017e-04 - val_loss: 0.6628\n",
            "Epoch 444/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 3.6403e-04 - val_loss: 0.6659\n",
            "Epoch 445/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 3.2978e-04 - val_loss: 0.6689\n",
            "Epoch 446/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 3.2885e-04 - val_loss: 0.6715\n",
            "Epoch 447/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 3.1716e-04 - val_loss: 0.6731\n",
            "Epoch 448/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.9946e-04 - val_loss: 0.6803\n",
            "Epoch 449/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 3.2640e-04 - val_loss: 0.6822\n",
            "Epoch 450/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 3.0278e-04 - val_loss: 0.6843\n",
            "Epoch 451/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 3.1311e-04 - val_loss: 0.6846\n",
            "Epoch 452/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 3.1836e-04 - val_loss: 0.6848\n",
            "Epoch 453/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 3.3435e-04 - val_loss: 0.6826\n",
            "Epoch 454/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.8746e-04 - val_loss: 0.6863\n",
            "Epoch 455/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.9953e-04 - val_loss: 0.6872\n",
            "Epoch 456/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 2.8467e-04 - val_loss: 0.6879\n",
            "Epoch 457/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.7619e-04 - val_loss: 0.6865\n",
            "Epoch 458/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.8734e-04 - val_loss: 0.6878\n",
            "Epoch 459/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 2.8012e-04 - val_loss: 0.6892\n",
            "Epoch 460/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 2.9253e-04 - val_loss: 0.6897\n",
            "Epoch 461/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.6420e-04 - val_loss: 0.6914\n",
            "Epoch 462/600\n",
            "14/14 [==============================] - 0s 19ms/step - loss: 2.8602e-04 - val_loss: 0.6909\n",
            "Epoch 463/600\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 2.6224e-04 - val_loss: 0.6940\n",
            "Epoch 464/600\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 2.7732e-04 - val_loss: 0.6975\n",
            "Epoch 465/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.6529e-04 - val_loss: 0.6976\n",
            "Epoch 466/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.6109e-04 - val_loss: 0.6987\n",
            "Epoch 467/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.6131e-04 - val_loss: 0.6985\n",
            "Epoch 468/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.9068e-04 - val_loss: 0.7020\n",
            "Epoch 469/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 3.1316e-04 - val_loss: 0.7002\n",
            "Epoch 470/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 2.5315e-04 - val_loss: 0.7068\n",
            "Epoch 471/600\n",
            "14/14 [==============================] - 0s 25ms/step - loss: 2.6184e-04 - val_loss: 0.7055\n",
            "Epoch 472/600\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 2.5152e-04 - val_loss: 0.7050\n",
            "Epoch 473/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.4586e-04 - val_loss: 0.7049\n",
            "Epoch 474/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 2.8630e-04 - val_loss: 0.7055\n",
            "Epoch 475/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.7109e-04 - val_loss: 0.7057\n",
            "Epoch 476/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 2.4738e-04 - val_loss: 0.7080\n",
            "Epoch 477/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 2.6231e-04 - val_loss: 0.7104\n",
            "Epoch 478/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.4002e-04 - val_loss: 0.7084\n",
            "Epoch 479/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.4130e-04 - val_loss: 0.7105\n",
            "Epoch 480/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 2.3900e-04 - val_loss: 0.7133\n",
            "Epoch 481/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 2.3670e-04 - val_loss: 0.7121\n",
            "Epoch 482/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.3662e-04 - val_loss: 0.7132\n",
            "Epoch 483/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.5270e-04 - val_loss: 0.7130\n",
            "Epoch 484/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.5774e-04 - val_loss: 0.7168\n",
            "Epoch 485/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 2.3807e-04 - val_loss: 0.7154\n",
            "Epoch 486/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.2597e-04 - val_loss: 0.7162\n",
            "Epoch 487/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.2851e-04 - val_loss: 0.7161\n",
            "Epoch 488/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 2.3721e-04 - val_loss: 0.7183\n",
            "Epoch 489/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 2.3907e-04 - val_loss: 0.7197\n",
            "Epoch 490/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 2.2461e-04 - val_loss: 0.7198\n",
            "Epoch 491/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 2.2881e-04 - val_loss: 0.7215\n",
            "Epoch 492/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.3051e-04 - val_loss: 0.7241\n",
            "Epoch 493/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.3251e-04 - val_loss: 0.7231\n",
            "Epoch 494/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.2944e-04 - val_loss: 0.7243\n",
            "Epoch 495/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.2674e-04 - val_loss: 0.7261\n",
            "Epoch 496/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 2.3650e-04 - val_loss: 0.7283\n",
            "Epoch 497/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.3138e-04 - val_loss: 0.7254\n",
            "Epoch 498/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.1984e-04 - val_loss: 0.7267\n",
            "Epoch 499/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.2227e-04 - val_loss: 0.7284\n",
            "Epoch 500/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.1248e-04 - val_loss: 0.7288\n",
            "Epoch 501/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 2.1117e-04 - val_loss: 0.7295\n",
            "Epoch 502/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 2.4862e-04 - val_loss: 0.7297\n",
            "Epoch 503/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.2658e-04 - val_loss: 0.7376\n",
            "Epoch 504/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.3147e-04 - val_loss: 0.7350\n",
            "Epoch 505/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.0630e-04 - val_loss: 0.7330\n",
            "Epoch 506/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.1202e-04 - val_loss: 0.7370\n",
            "Epoch 507/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 2.1201e-04 - val_loss: 0.7369\n",
            "Epoch 508/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.3028e-04 - val_loss: 0.7386\n",
            "Epoch 509/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.1611e-04 - val_loss: 0.7408\n",
            "Epoch 510/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.1343e-04 - val_loss: 0.7399\n",
            "Epoch 511/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.1136e-04 - val_loss: 0.7412\n",
            "Epoch 512/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 2.1023e-04 - val_loss: 0.7442\n",
            "Epoch 513/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 2.1193e-04 - val_loss: 0.7442\n",
            "Epoch 514/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.0429e-04 - val_loss: 0.7435\n",
            "Epoch 515/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 1.9683e-04 - val_loss: 0.7439\n",
            "Epoch 516/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 2.0708e-04 - val_loss: 0.7454\n",
            "Epoch 517/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.0256e-04 - val_loss: 0.7484\n",
            "Epoch 518/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.0984e-04 - val_loss: 0.7441\n",
            "Epoch 519/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 1.9956e-04 - val_loss: 0.7458\n",
            "Epoch 520/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 1.9796e-04 - val_loss: 0.7453\n",
            "Epoch 521/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.0124e-04 - val_loss: 0.7477\n",
            "Epoch 522/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 1.9688e-04 - val_loss: 0.7470\n",
            "Epoch 523/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 1.9644e-04 - val_loss: 0.7487\n",
            "Epoch 524/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 1.9969e-04 - val_loss: 0.7467\n",
            "Epoch 525/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 1.9306e-04 - val_loss: 0.7499\n",
            "Epoch 526/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.0805e-04 - val_loss: 0.7499\n",
            "Epoch 527/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 1.8938e-04 - val_loss: 0.7479\n",
            "Epoch 528/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 1.9492e-04 - val_loss: 0.7504\n",
            "Epoch 529/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 1.9380e-04 - val_loss: 0.7511\n",
            "Epoch 530/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 1.9310e-04 - val_loss: 0.7491\n",
            "Epoch 531/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 1.9537e-04 - val_loss: 0.7497\n",
            "Epoch 532/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.0979e-04 - val_loss: 0.7543\n",
            "Epoch 533/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 1.8339e-04 - val_loss: 0.7551\n",
            "Epoch 534/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 1.9948e-04 - val_loss: 0.7562\n",
            "Epoch 535/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 1.8729e-04 - val_loss: 0.7524\n",
            "Epoch 536/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 2.0275e-04 - val_loss: 0.7524\n",
            "Epoch 537/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 2.2372e-04 - val_loss: 0.7536\n",
            "Epoch 538/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 1.8930e-04 - val_loss: 0.7501\n",
            "Epoch 539/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 1.9126e-04 - val_loss: 0.7520\n",
            "Epoch 540/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 1.8791e-04 - val_loss: 0.7544\n",
            "Epoch 541/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 1.8539e-04 - val_loss: 0.7532\n",
            "Epoch 542/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 1.9107e-04 - val_loss: 0.7530\n",
            "Epoch 543/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 1.9340e-04 - val_loss: 0.7545\n",
            "Epoch 544/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 1.8325e-04 - val_loss: 0.7525\n",
            "Epoch 545/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 1.8730e-04 - val_loss: 0.7547\n",
            "Epoch 546/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 1.7807e-04 - val_loss: 0.7562\n",
            "Epoch 547/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 1.8023e-04 - val_loss: 0.7523\n",
            "Epoch 548/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 1.8485e-04 - val_loss: 0.7558\n",
            "Epoch 549/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 1.8161e-04 - val_loss: 0.7579\n",
            "Epoch 550/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 1.8416e-04 - val_loss: 0.7585\n",
            "Epoch 551/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 1.8946e-04 - val_loss: 0.7568\n",
            "Epoch 552/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 1.9078e-04 - val_loss: 0.7540\n",
            "Epoch 553/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 1.7363e-04 - val_loss: 0.7586\n",
            "Epoch 554/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 1.7594e-04 - val_loss: 0.7621\n",
            "Epoch 555/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 1.7872e-04 - val_loss: 0.7632\n",
            "Epoch 556/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 1.8815e-04 - val_loss: 0.7630\n",
            "Epoch 557/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 1.6887e-04 - val_loss: 0.7659\n",
            "Epoch 558/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 1.8942e-04 - val_loss: 0.7631\n",
            "Epoch 559/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 2.0194e-04 - val_loss: 0.7613\n",
            "Epoch 560/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 1.6947e-04 - val_loss: 0.7615\n",
            "Epoch 561/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 1.8238e-04 - val_loss: 0.7624\n",
            "Epoch 562/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 1.6679e-04 - val_loss: 0.7649\n",
            "Epoch 563/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 1.7069e-04 - val_loss: 0.7630\n",
            "Epoch 564/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 1.7205e-04 - val_loss: 0.7655\n",
            "Epoch 565/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 1.7034e-04 - val_loss: 0.7671\n",
            "Epoch 566/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 1.7582e-04 - val_loss: 0.7660\n",
            "Epoch 567/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 1.7169e-04 - val_loss: 0.7670\n",
            "Epoch 568/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 1.6007e-04 - val_loss: 0.7654\n",
            "Epoch 569/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 1.8963e-04 - val_loss: 0.7710\n",
            "Epoch 570/600\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.7670e-04 - val_loss: 0.7709\n",
            "Epoch 571/600\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.6046e-04 - val_loss: 0.7700\n",
            "Epoch 572/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 1.6165e-04 - val_loss: 0.7669\n",
            "Epoch 573/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 1.7074e-04 - val_loss: 0.7686\n",
            "Epoch 574/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 1.7071e-04 - val_loss: 0.7643\n",
            "Epoch 575/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 1.5560e-04 - val_loss: 0.7688\n",
            "Epoch 576/600\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.6784e-04 - val_loss: 0.7654\n",
            "Epoch 577/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 1.6200e-04 - val_loss: 0.7667\n",
            "Epoch 578/600\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.6177e-04 - val_loss: 0.7716\n",
            "Epoch 579/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 1.5819e-04 - val_loss: 0.7693\n",
            "Epoch 580/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 1.8790e-04 - val_loss: 0.7706\n",
            "Epoch 581/600\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.6209e-04 - val_loss: 0.7728\n",
            "Epoch 582/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 1.8656e-04 - val_loss: 0.7688\n",
            "Epoch 583/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 1.6513e-04 - val_loss: 0.7724\n",
            "Epoch 584/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 1.5609e-04 - val_loss: 0.7771\n",
            "Epoch 585/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 1.5981e-04 - val_loss: 0.7770\n",
            "Epoch 586/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 1.5168e-04 - val_loss: 0.7728\n",
            "Epoch 587/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 1.5802e-04 - val_loss: 0.7732\n",
            "Epoch 588/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 1.5654e-04 - val_loss: 0.7738\n",
            "Epoch 589/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 1.7148e-04 - val_loss: 0.7758\n",
            "Epoch 590/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 1.6242e-04 - val_loss: 0.7781\n",
            "Epoch 591/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 1.4962e-04 - val_loss: 0.7758\n",
            "Epoch 592/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 1.4912e-04 - val_loss: 0.7766\n",
            "Epoch 593/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 1.5238e-04 - val_loss: 0.7752\n",
            "Epoch 594/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 1.4579e-04 - val_loss: 0.7777\n",
            "Epoch 595/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 1.5383e-04 - val_loss: 0.7781\n",
            "Epoch 596/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 1.6858e-04 - val_loss: 0.7796\n",
            "Epoch 597/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 1.9615e-04 - val_loss: 0.7743\n",
            "Epoch 598/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 1.5013e-04 - val_loss: 0.7754\n",
            "Epoch 599/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 1.5151e-04 - val_loss: 0.7776\n",
            "Epoch 600/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 1.4750e-04 - val_loss: 0.7819\n",
            "CPU times: user 46.5 s, sys: 1.94 s, total: 48.4 s\n",
            "Wall time: 1min 21s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa11d873910>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "# https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network\n",
        "# https://datascience.stackexchange.com/questions/18414/are-there-any-rules-for-choosing-the-size-of-a-mini-batch\n",
        "\n",
        "model.fit(x=X_train, y=y_train, epochs=600, validation_data=(X_test, y_test), verbose=1) #(includes both progress bar and one line per epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4KH5548AHuL"
      },
      "outputs": [],
      "source": [
        "# model.history.history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFS28egAAHuM"
      },
      "outputs": [],
      "source": [
        "model_loss = pd.DataFrame(model.history.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3IHhaiUAHuM"
      },
      "outputs": [],
      "source": [
        "# model_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "F5po8lngAHuM",
        "outputId": "7f93d50b-7caf-457a-c164-5240605bcf81"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fa11ff12d90>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_loss.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy9B8OVJAHuM"
      },
      "source": [
        "## Example Two: Early Stopping\n",
        "\n",
        "> Indented block\n",
        "\n",
        "\n",
        "\n",
        "We obviously trained too much! Let's use early stopping to track the val_loss and stop training once it begins increasing too much!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0aqgRi_AHuM"
      },
      "outputs": [],
      "source": [
        "model = Sequential() #creating a sequential model\n",
        "model.add(Dense(units=30,activation='relu')) #adding a layer with 30 neurons and relu as activation function\n",
        "model.add(Dense(units=15,activation='relu'))  #adding a layer with 15 neurons and relu as activation function\n",
        "model.add(Dense(units=1,activation='sigmoid'))  #adding a layer with 1 neurons and sigmoid as activation function\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam')  #compiling our model using adam optimizer (results of the Adam optimizer are generally better than every other optimization algorithms, have faster computation time, and require fewer parameters for tuning)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqL43or1AHuM"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping #impoting the required functions from libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX4nrAjGAHuM"
      },
      "source": [
        "Stop training when a monitored quantity has stopped improving.\n",
        "\n",
        "    Arguments:\n",
        "        monitor: Quantity to be monitored.\n",
        "        min_delta: Minimum change in the monitored quantity\n",
        "            to qualify as an improvement, i.e. an absolute\n",
        "            change of less than min_delta, will count as no\n",
        "            improvement.\n",
        "        patience: Number of epochs with no improvement\n",
        "            after which training will be stopped.\n",
        "        verbose: verbosity mode.\n",
        "        mode: One of `{\"auto\", \"min\", \"max\"}`. In `min` mode,\n",
        "            training will stop when the quantity\n",
        "            monitored has stopped decreasing; in `max`\n",
        "            mode it will stop when the quantity\n",
        "            monitored has stopped increasing; in `auto`\n",
        "            mode, the direction is automatically inferred\n",
        "            from the name of the monitored quantity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_HK1dBqAHuM"
      },
      "outputs": [],
      "source": [
        "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)\n",
        "#(training will terminate only if there is no improvement in the monitor performance measure for 25 epochs)\n",
        "# monitor='val_loss' = to use validation loss as performance measure to terminate the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTA6qYuGAHuM",
        "outputId": "37b028c0-0638-4a25-f0f5-c2bf2d2b557d",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/600\n",
            "14/14 [==============================] - 2s 26ms/step - loss: 0.6811 - val_loss: 0.6690\n",
            "Epoch 2/600\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.6563 - val_loss: 0.6455\n",
            "Epoch 3/600\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.6321 - val_loss: 0.6206\n",
            "Epoch 4/600\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.6048 - val_loss: 0.5911\n",
            "Epoch 5/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.5743 - val_loss: 0.5564\n",
            "Epoch 6/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.5389 - val_loss: 0.5198\n",
            "Epoch 7/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.5009 - val_loss: 0.4790\n",
            "Epoch 8/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.4599 - val_loss: 0.4370\n",
            "Epoch 9/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.4198 - val_loss: 0.3922\n",
            "Epoch 10/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.3773 - val_loss: 0.3511\n",
            "Epoch 11/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.3382 - val_loss: 0.3091\n",
            "Epoch 12/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.2990 - val_loss: 0.2742\n",
            "Epoch 13/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.2678 - val_loss: 0.2454\n",
            "Epoch 14/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.2418 - val_loss: 0.2222\n",
            "Epoch 15/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.2224 - val_loss: 0.2041\n",
            "Epoch 16/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.2081 - val_loss: 0.1930\n",
            "Epoch 17/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1937 - val_loss: 0.1800\n",
            "Epoch 18/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1834 - val_loss: 0.1697\n",
            "Epoch 19/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1712 - val_loss: 0.1613\n",
            "Epoch 20/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1630 - val_loss: 0.1532\n",
            "Epoch 21/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1560 - val_loss: 0.1474\n",
            "Epoch 22/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1488 - val_loss: 0.1416\n",
            "Epoch 23/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1413 - val_loss: 0.1416\n",
            "Epoch 24/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1367 - val_loss: 0.1340\n",
            "Epoch 25/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1341 - val_loss: 0.1290\n",
            "Epoch 26/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1282 - val_loss: 0.1288\n",
            "Epoch 27/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1216 - val_loss: 0.1256\n",
            "Epoch 28/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1184 - val_loss: 0.1254\n",
            "Epoch 29/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1194 - val_loss: 0.1210\n",
            "Epoch 30/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1128 - val_loss: 0.1174\n",
            "Epoch 31/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1063 - val_loss: 0.1177\n",
            "Epoch 32/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1031 - val_loss: 0.1171\n",
            "Epoch 33/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0997 - val_loss: 0.1148\n",
            "Epoch 34/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0954 - val_loss: 0.1133\n",
            "Epoch 35/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0938 - val_loss: 0.1107\n",
            "Epoch 36/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0916 - val_loss: 0.1112\n",
            "Epoch 37/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0898 - val_loss: 0.1118\n",
            "Epoch 38/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0876 - val_loss: 0.1105\n",
            "Epoch 39/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0849 - val_loss: 0.1085\n",
            "Epoch 40/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0835 - val_loss: 0.1075\n",
            "Epoch 41/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0824 - val_loss: 0.1083\n",
            "Epoch 42/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0881 - val_loss: 0.1096\n",
            "Epoch 43/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0794 - val_loss: 0.1099\n",
            "Epoch 44/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0769 - val_loss: 0.1058\n",
            "Epoch 45/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0746 - val_loss: 0.1082\n",
            "Epoch 46/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0754 - val_loss: 0.1069\n",
            "Epoch 47/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0733 - val_loss: 0.1064\n",
            "Epoch 48/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0717 - val_loss: 0.1059\n",
            "Epoch 49/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0703 - val_loss: 0.1072\n",
            "Epoch 50/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0693 - val_loss: 0.1048\n",
            "Epoch 51/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0697 - val_loss: 0.1072\n",
            "Epoch 52/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0686 - val_loss: 0.1042\n",
            "Epoch 53/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0673 - val_loss: 0.1055\n",
            "Epoch 54/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0668 - val_loss: 0.1060\n",
            "Epoch 55/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0653 - val_loss: 0.1078\n",
            "Epoch 56/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0648 - val_loss: 0.1070\n",
            "Epoch 57/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0643 - val_loss: 0.1068\n",
            "Epoch 58/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0635 - val_loss: 0.1093\n",
            "Epoch 59/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0640 - val_loss: 0.1038\n",
            "Epoch 60/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0630 - val_loss: 0.1063\n",
            "Epoch 61/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0617 - val_loss: 0.1049\n",
            "Epoch 62/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0613 - val_loss: 0.1102\n",
            "Epoch 63/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0631 - val_loss: 0.1045\n",
            "Epoch 64/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0600 - val_loss: 0.1129\n",
            "Epoch 65/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0665 - val_loss: 0.1079\n",
            "Epoch 66/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0608 - val_loss: 0.1099\n",
            "Epoch 67/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0597 - val_loss: 0.1068\n",
            "Epoch 68/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0599 - val_loss: 0.1052\n",
            "Epoch 69/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0594 - val_loss: 0.1101\n",
            "Epoch 70/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0584 - val_loss: 0.1064\n",
            "Epoch 71/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0577 - val_loss: 0.1096\n",
            "Epoch 72/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0575 - val_loss: 0.1098\n",
            "Epoch 73/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0573 - val_loss: 0.1118\n",
            "Epoch 74/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0569 - val_loss: 0.1076\n",
            "Epoch 75/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0578 - val_loss: 0.1129\n",
            "Epoch 76/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0578 - val_loss: 0.1088\n",
            "Epoch 77/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0569 - val_loss: 0.1114\n",
            "Epoch 78/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0559 - val_loss: 0.1097\n",
            "Epoch 79/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0556 - val_loss: 0.1121\n",
            "Epoch 80/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0575 - val_loss: 0.1133\n",
            "Epoch 81/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0558 - val_loss: 0.1123\n",
            "Epoch 82/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0544 - val_loss: 0.1130\n",
            "Epoch 83/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0555 - val_loss: 0.1127\n",
            "Epoch 84/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0551 - val_loss: 0.1120\n",
            "Epoch 84: early stopping\n",
            "CPU times: user 8.66 s, sys: 302 ms, total: 8.96 s\n",
            "Wall time: 9.05 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa11e587790>"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "model.fit(x=X_train, y=y_train, epochs=600, validation_data=(X_test, y_test), verbose=1, callbacks=[early_stop])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "nkZKefbeAHuM",
        "outputId": "653c466b-912d-41d4-f0f1-4d25903e7dd1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fa11e37e6d0>"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_loss = pd.DataFrame(model.history.history)\n",
        "model_loss.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIJu4cWVAHuN"
      },
      "source": [
        "## Example Three: Adding in DropOut Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtoo67-dAHuN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_i4JtnYvAHuN"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(units=30,activation='relu'))\n",
        "model.add(Dropout(0.5)) #(The dropout rate is set to 50%, meaning one in two inputs will be randomly excluded from each update cycle.)\n",
        "\n",
        "model.add(Dense(units=15,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(units=1,activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1czSg7UWAHuN",
        "outputId": "c8d9f2c3-7d77-4979-8f6a-8229cfb2b872",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/600\n",
            "14/14 [==============================] - 1s 15ms/step - loss: 0.7190 - val_loss: 0.6760\n",
            "Epoch 2/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.6759 - val_loss: 0.6542\n",
            "Epoch 3/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.6591 - val_loss: 0.6346\n",
            "Epoch 4/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.6586 - val_loss: 0.6201\n",
            "Epoch 5/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.6382 - val_loss: 0.6064\n",
            "Epoch 6/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.6107 - val_loss: 0.5872\n",
            "Epoch 7/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.6146 - val_loss: 0.5680\n",
            "Epoch 8/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.5784 - val_loss: 0.5499\n",
            "Epoch 9/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.5736 - val_loss: 0.5253\n",
            "Epoch 10/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.5589 - val_loss: 0.5033\n",
            "Epoch 11/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.5355 - val_loss: 0.4789\n",
            "Epoch 12/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.5219 - val_loss: 0.4535\n",
            "Epoch 13/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.4861 - val_loss: 0.4235\n",
            "Epoch 14/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.4996 - val_loss: 0.3982\n",
            "Epoch 15/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.4727 - val_loss: 0.3770\n",
            "Epoch 16/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.4311 - val_loss: 0.3482\n",
            "Epoch 17/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.4073 - val_loss: 0.3239\n",
            "Epoch 18/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.4264 - val_loss: 0.3035\n",
            "Epoch 19/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.3749 - val_loss: 0.2807\n",
            "Epoch 20/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.3790 - val_loss: 0.2672\n",
            "Epoch 21/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.3595 - val_loss: 0.2514\n",
            "Epoch 22/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.3427 - val_loss: 0.2404\n",
            "Epoch 23/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.3140 - val_loss: 0.2264\n",
            "Epoch 24/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.3046 - val_loss: 0.2106\n",
            "Epoch 25/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.2981 - val_loss: 0.2047\n",
            "Epoch 26/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.3164 - val_loss: 0.2041\n",
            "Epoch 27/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.2870 - val_loss: 0.1959\n",
            "Epoch 28/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.2658 - val_loss: 0.1879\n",
            "Epoch 29/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.2780 - val_loss: 0.1807\n",
            "Epoch 30/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.2729 - val_loss: 0.1802\n",
            "Epoch 31/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.2534 - val_loss: 0.1693\n",
            "Epoch 32/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.2681 - val_loss: 0.1666\n",
            "Epoch 33/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.2409 - val_loss: 0.1597\n",
            "Epoch 34/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.2376 - val_loss: 0.1558\n",
            "Epoch 35/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.2506 - val_loss: 0.1541\n",
            "Epoch 36/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.2339 - val_loss: 0.1507\n",
            "Epoch 37/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.2160 - val_loss: 0.1410\n",
            "Epoch 38/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.2391 - val_loss: 0.1428\n",
            "Epoch 39/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.2192 - val_loss: 0.1437\n",
            "Epoch 40/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.2048 - val_loss: 0.1411\n",
            "Epoch 41/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1962 - val_loss: 0.1376\n",
            "Epoch 42/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1927 - val_loss: 0.1308\n",
            "Epoch 43/600\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1983 - val_loss: 0.1408\n",
            "Epoch 44/600\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1993 - val_loss: 0.1291\n",
            "Epoch 45/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.1540 - val_loss: 0.1287\n",
            "Epoch 46/600\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1762 - val_loss: 0.1218\n",
            "Epoch 47/600\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1719 - val_loss: 0.1239\n",
            "Epoch 48/600\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1850 - val_loss: 0.1249\n",
            "Epoch 49/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.1649 - val_loss: 0.1225\n",
            "Epoch 50/600\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1827 - val_loss: 0.1172\n",
            "Epoch 51/600\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1831 - val_loss: 0.1202\n",
            "Epoch 52/600\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1875 - val_loss: 0.1158\n",
            "Epoch 53/600\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1652 - val_loss: 0.1177\n",
            "Epoch 54/600\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1398 - val_loss: 0.1108\n",
            "Epoch 55/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.1731 - val_loss: 0.1115\n",
            "Epoch 56/600\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1509 - val_loss: 0.1322\n",
            "Epoch 57/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.1673 - val_loss: 0.1164\n",
            "Epoch 58/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.1519 - val_loss: 0.1133\n",
            "Epoch 59/600\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1325 - val_loss: 0.1239\n",
            "Epoch 60/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.1467 - val_loss: 0.1109\n",
            "Epoch 61/600\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1480 - val_loss: 0.1084\n",
            "Epoch 62/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.1513 - val_loss: 0.1100\n",
            "Epoch 63/600\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1383 - val_loss: 0.1095\n",
            "Epoch 64/600\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.1433 - val_loss: 0.1061\n",
            "Epoch 65/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1423 - val_loss: 0.1050\n",
            "Epoch 66/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1388 - val_loss: 0.1058\n",
            "Epoch 67/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1258 - val_loss: 0.1088\n",
            "Epoch 68/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1382 - val_loss: 0.1124\n",
            "Epoch 69/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1490 - val_loss: 0.1031\n",
            "Epoch 70/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1202 - val_loss: 0.1008\n",
            "Epoch 71/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1520 - val_loss: 0.1074\n",
            "Epoch 72/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1355 - val_loss: 0.1025\n",
            "Epoch 73/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1355 - val_loss: 0.1017\n",
            "Epoch 74/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1304 - val_loss: 0.1049\n",
            "Epoch 75/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1279 - val_loss: 0.1047\n",
            "Epoch 76/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1300 - val_loss: 0.0999\n",
            "Epoch 77/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1494 - val_loss: 0.1028\n",
            "Epoch 78/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1263 - val_loss: 0.1027\n",
            "Epoch 79/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1262 - val_loss: 0.1006\n",
            "Epoch 80/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1094 - val_loss: 0.1012\n",
            "Epoch 81/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1350 - val_loss: 0.1051\n",
            "Epoch 82/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1304 - val_loss: 0.1038\n",
            "Epoch 83/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1032 - val_loss: 0.1021\n",
            "Epoch 84/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1289 - val_loss: 0.1018\n",
            "Epoch 85/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1217 - val_loss: 0.1002\n",
            "Epoch 86/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1392 - val_loss: 0.1039\n",
            "Epoch 87/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1147 - val_loss: 0.0973\n",
            "Epoch 88/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1038 - val_loss: 0.1044\n",
            "Epoch 89/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1081 - val_loss: 0.1008\n",
            "Epoch 90/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1183 - val_loss: 0.1005\n",
            "Epoch 91/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1184 - val_loss: 0.0971\n",
            "Epoch 92/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1139 - val_loss: 0.1113\n",
            "Epoch 93/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1040 - val_loss: 0.1147\n",
            "Epoch 94/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1151 - val_loss: 0.0991\n",
            "Epoch 95/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1139 - val_loss: 0.0971\n",
            "Epoch 96/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1126 - val_loss: 0.1020\n",
            "Epoch 97/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0984 - val_loss: 0.0961\n",
            "Epoch 98/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1046 - val_loss: 0.1014\n",
            "Epoch 99/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1216 - val_loss: 0.1055\n",
            "Epoch 100/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1200 - val_loss: 0.0948\n",
            "Epoch 101/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1164 - val_loss: 0.1033\n",
            "Epoch 102/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1002 - val_loss: 0.1036\n",
            "Epoch 103/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0860 - val_loss: 0.1000\n",
            "Epoch 104/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1124 - val_loss: 0.0968\n",
            "Epoch 105/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1191 - val_loss: 0.1113\n",
            "Epoch 106/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1002 - val_loss: 0.1074\n",
            "Epoch 107/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0815 - val_loss: 0.1049\n",
            "Epoch 108/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1022 - val_loss: 0.1024\n",
            "Epoch 109/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0944 - val_loss: 0.0969\n",
            "Epoch 110/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1086 - val_loss: 0.0970\n",
            "Epoch 111/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1102 - val_loss: 0.1068\n",
            "Epoch 112/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0954 - val_loss: 0.0975\n",
            "Epoch 113/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0995 - val_loss: 0.1019\n",
            "Epoch 114/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0872 - val_loss: 0.1020\n",
            "Epoch 115/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1060 - val_loss: 0.1012\n",
            "Epoch 116/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0981 - val_loss: 0.1086\n",
            "Epoch 117/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.1056 - val_loss: 0.0941\n",
            "Epoch 118/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1007 - val_loss: 0.1052\n",
            "Epoch 119/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0998 - val_loss: 0.0977\n",
            "Epoch 120/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1004 - val_loss: 0.1008\n",
            "Epoch 121/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0751 - val_loss: 0.1055\n",
            "Epoch 122/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0836 - val_loss: 0.1028\n",
            "Epoch 123/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0975 - val_loss: 0.1000\n",
            "Epoch 124/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0931 - val_loss: 0.1127\n",
            "Epoch 125/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0952 - val_loss: 0.1008\n",
            "Epoch 126/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0854 - val_loss: 0.0993\n",
            "Epoch 127/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0964 - val_loss: 0.1000\n",
            "Epoch 128/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0809 - val_loss: 0.0974\n",
            "Epoch 129/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0756 - val_loss: 0.1038\n",
            "Epoch 130/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1162 - val_loss: 0.0993\n",
            "Epoch 131/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1112 - val_loss: 0.1041\n",
            "Epoch 132/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0909 - val_loss: 0.1066\n",
            "Epoch 133/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0839 - val_loss: 0.1009\n",
            "Epoch 134/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0980 - val_loss: 0.1005\n",
            "Epoch 135/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0769 - val_loss: 0.1031\n",
            "Epoch 136/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0575 - val_loss: 0.0981\n",
            "Epoch 137/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0800 - val_loss: 0.0970\n",
            "Epoch 138/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0868 - val_loss: 0.0965\n",
            "Epoch 139/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0827 - val_loss: 0.0997\n",
            "Epoch 140/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0945 - val_loss: 0.1022\n",
            "Epoch 141/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0876 - val_loss: 0.0980\n",
            "Epoch 142/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0784 - val_loss: 0.1042\n",
            "Epoch 142: early stopping\n",
            "CPU times: user 13.4 s, sys: 496 ms, total: 13.9 s\n",
            "Wall time: 14.1 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa1252841c0>"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "model.fit(x=X_train, y=y_train, epochs=600, validation_data=(X_test, y_test), verbose=1, callbacks=[early_stop])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "BDqkyt96AHuN",
        "outputId": "575b2ba5-5175-462a-f376-a27d95748a6a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fa1251342b0>"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_loss = pd.DataFrame(model.history.history)\n",
        "model_loss.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iSXGFvyAHuN"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmmDhGuXAHuN",
        "outputId": "081e3f4d-a922-469a-8134-ad616e017514"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        }
      ],
      "source": [
        "#predictions = model.predict_classes(X_test) -> no longer supported in keras\n",
        "\n",
        "predictions = (model.predict(X_test) > 0.5).astype(\"int32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAHCEHrSAHuN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix #importing the required features from the library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYoZPNHsAHuN",
        "outputId": "d07b96d9-bb54-41aa-eca4-e2488455af2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.98      0.96        55\n",
            "           1       0.99      0.97      0.98        88\n",
            "\n",
            "    accuracy                           0.97       143\n",
            "   macro avg       0.97      0.97      0.97       143\n",
            "weighted avg       0.97      0.97      0.97       143\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# https://en.wikipedia.org/wiki/Precision_and_recall\n",
        "print(classification_report(y_test,predictions)) #Classification report is used to measure the quality of predictions from a classification algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWIcBJRZAHuN",
        "outputId": "c82a36e5-b83e-47eb-ecf3-a52cb0c60253"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[54  1]\n",
            " [ 3 85]]\n"
          ]
        }
      ],
      "source": [
        "print(confusion_matrix(y_test,predictions)) #printing a confusion matrix between y_test and predictions"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}